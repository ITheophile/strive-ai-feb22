{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torchvision import datasets, transforms\nimport torch\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\nimport torch.optim as optim\ntorch.manual_seed(0)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-01T19:24:05.606377Z","iopub.execute_input":"2022-06-01T19:24:05.606976Z","iopub.status.idle":"2022-06-01T19:24:05.617820Z","shell.execute_reply.started":"2022-06-01T19:24:05.606922Z","shell.execute_reply":"2022-06-01T19:24:05.616404Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"train_transform = transforms.Compose([\n                                      transforms.RandomRotation(30),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.5,),(0.5,))\n                                       ])\n\ntest_transform = transforms.Compose([ \n                                     transforms.ToTensor(), \n                                     transforms.Normalize((0.5,),(0.5,))\n                                     ])\n# Download and load the training data\ntrainset = datasets.FashionMNIST(\n    '.', download=True, train=True, transform=train_transform)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=64, shuffle=True)\n\n# Download and load the test data\ntestset = datasets.FashionMNIST(\n    '.', download=True, train=False, transform=test_transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T19:24:05.712892Z","iopub.execute_input":"2022-06-01T19:24:05.713339Z","iopub.status.idle":"2022-06-01T19:24:05.867263Z","shell.execute_reply.started":"2022-06-01T19:24:05.713305Z","shell.execute_reply":"2022-06-01T19:24:05.865684Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"imgs, labels = iter(testloader).next()\nimgs.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-01T19:24:05.869122Z","iopub.execute_input":"2022-06-01T19:24:05.869507Z","iopub.status.idle":"2022-06-01T19:24:05.890890Z","shell.execute_reply.started":"2022-06-01T19:24:05.869473Z","shell.execute_reply":"2022-06-01T19:24:05.889809Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"input_size = 28*28\nhiddensizes = [512, 256, 128, 64]\noutput_size = 10\npercent_drop = 0.10\n\nneuralnet = nn.Sequential(\n    nn.Linear(input_size, hiddensizes[0]),\n    nn.Dropout(percent_drop),\n    nn.BatchNorm1d(hiddensizes[0]),\n    nn.ReLU(),\n\n    nn.Linear(hiddensizes[0], hiddensizes[1]),\n    nn.Dropout(percent_drop),\n    nn.BatchNorm1d(hiddensizes[1]),\n    nn.ReLU(),\n\n    nn.Linear(hiddensizes[1], hiddensizes[2]),\n    nn.Dropout(percent_drop),\n    nn.BatchNorm1d(hiddensizes[2]),\n    nn.ReLU(),\n\n    nn.Linear(hiddensizes[2], hiddensizes[3]),\n    nn.Dropout(percent_drop),\n    nn.BatchNorm1d(hiddensizes[3]),\n    nn.ReLU(),\n\n    nn.Linear(hiddensizes[3], output_size),\n    # nn.LogSoftmax(dim=1)\n\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T19:24:05.892216Z","iopub.execute_input":"2022-06-01T19:24:05.893111Z","iopub.status.idle":"2022-06-01T19:24:05.906322Z","shell.execute_reply.started":"2022-06-01T19:24:05.893075Z","shell.execute_reply":"2022-06-01T19:24:05.905197Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\nepochs = 100  # Many more epochs are needed(minimum 100)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(neuralnet.parameters(), lr=learning_rate)\n\ntrain_losses = []\ntest_losses = []\ntrain_accuracies = []\ntest_accuracies = []\nbenchmark_accuracy = 0.85\nfor epoch in tqdm(range(epochs)):\n    train_batch_accuracies = []\n    train_batch_losses = []\n    # training\n    for x_train_batch, y_train_batch in trainloader:\n\n        optimizer.zero_grad()\n\n        # forward pass\n        logits = neuralnet(x_train_batch.view(x_train_batch.shape[0], -1))\n        train_preds = torch.argmax(logits.detach(), dim=1)\n\n        # loss\n        train_loss = criterion(logits, y_train_batch)\n        train_batch_losses.append(train_loss.item())\n\n        # train accuracy\n        train_batch_accuracies.append(\n            accuracy_score(y_train_batch, train_preds))\n\n        # backward pass\n        train_loss.backward()\n\n        optimizer.step()\n\n    # mean loss (all batches losses divided by the total number of batches)\n    train_losses.append(sum(train_batch_losses) / len(trainloader))\n    train_accuracies.append(sum(train_batch_accuracies) / len(trainloader))\n    # mean accuracies\n\n    # validation\n    neuralnet.eval()\n    with torch.no_grad():\n        test_batch_accuracies = []\n        test_batch_losses = []\n\n        for x_test_batch, y_test_batch in testloader:\n\n            # logits\n            test_logits = neuralnet(\n                x_test_batch.view(x_test_batch.shape[0], -1))\n\n            # predictions\n            test_preds = torch.argmax(test_logits, dim=1)\n\n            # accuracy\n            test_batch_accuracies.append(\n                accuracy_score(y_test_batch, test_preds))\n\n            # loss\n            test_loss = criterion(test_logits, y_test_batch)\n            test_batch_losses.append(test_loss.item())\n\n        # mean accuracy for each epoch\n        test_accuracies.append(sum(test_batch_accuracies)/len(testloader))\n\n        # mean loss for each epoch\n        test_losses.append(sum(test_batch_losses)/len(testloader))\n\n        # saving best model\n        # is current mean score (mean per epoch) greater than or equal to the benchmark?\n        if test_accuracies[-1] > benchmark_accuracy:\n            # save model\n            torch.save(neuralnet.state_dict(), './model.pth')\n\n            # update benckmark\n            benchmark_accuracy = test_accuracies[-1]\n\n    neuralnet.train()\n\n\n# Plots\nx_epochs = list(range(epochs))\nplt.figure(figsize=(15, 6))\nplt.subplot(1, 2, 1)\nplt.plot(x_epochs, train_losses, marker='o', label='train')\nplt.plot(x_epochs, test_losses, marker='o', label='test')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x_epochs, train_accuracies, marker='o', label='train')\nplt.plot(x_epochs, test_accuracies, marker='o', label='test')\nplt.axhline(benchmark_accuracy, c='grey', ls='--',\n            label=f'benchmark_accuracy({benchmark_accuracy :.2f})')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.savefig('./learning_curve.png', dpi = 200)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T19:24:05.907887Z","iopub.execute_input":"2022-06-01T19:24:05.908461Z","iopub.status.idle":"2022-06-01T21:01:27.528014Z","shell.execute_reply.started":"2022-06-01T19:24:05.908414Z","shell.execute_reply":"2022-06-01T21:01:27.526409Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"<a href='model.pth'> Download File </a>\n\n<a href='learning_curve.png'> Download File </a>","metadata":{}}]}